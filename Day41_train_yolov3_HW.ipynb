{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Day41_train_yolov3_HW.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CG77DrrB2CrU"},"source":["## 作業\n","\n","1. 如何使用已經訓練好的模型？\n","2. 依照 https://github.com/qqwweee/keras-yolo3 的程式碼，請敘述，訓練模型時，資料集的格式是什麼？具體一點的說，要提供什麼格式的文件來描述資料集的圖片以及 bboxes 的信息呢？\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NCEP-DG0VxlV","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1596506232978,"user_tz":-480,"elapsed":1877,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}},"outputId":"80e0007c-7966-47e3-d40e-003503b28c48"},"source":["%tensorflow_version 1.x # 確保 colob 中使用的 tensorflow 是 1.x 版本而不是 tensorflow 2\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.x # 確保 colob 中使用的 tensorflow 是 1.x 版本而不是 tensorflow 2`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eXT7SQe0KQxv","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1596506235735,"user_tz":-480,"elapsed":3189,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}},"outputId":"5a56346c-a98a-4f0f-aa61-343e59294725"},"source":["pip install keras==2.2.4 # 需要安裝 keras 2.2.4 的版本"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.18.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras==2.2.4) (1.0.8)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vELO-PTVxAtm","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1596506241628,"user_tz":-480,"elapsed":598,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}},"outputId":"2019be22-1dbe-44a3-ed52-8c70cd641a9b"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive') # 將 google drive 掛載在 colob，\n","# 下載基於 keras 的 yolov3 程式碼\n","%cd '/content/gdrive/My Drive/CV Course'\n","# !git clone https://github.com/qqwweee/keras-yolo3 # 如果之前已經下載過就可以註解掉\n","%cd keras-yolo3"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/CV Course\n","/content/gdrive/My Drive/CV Course/keras-yolo3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k0MyCUeRuARA","colab":{},"executionInfo":{"status":"ok","timestamp":1596506246561,"user_tz":-480,"elapsed":820,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}}},"source":["from PIL import Image\n","image = Image.open('dog.jpg') "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ooyy53TW5c0L","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596506246565,"user_tz":-480,"elapsed":523,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}},"outputId":"72b39f1f-db5d-4e76-9f60-643d3b79a608"},"source":["import os\n","if not os.path.exists(\"model_data/yolo.h5\"):\n","  # 下載 yolov3 的網路權重，並且把權重轉換為 keras 能夠讀取的格式\n","  print(\"Model doesn't exist, downloading...\")\n","  os.system(\"wget https://pjreddie.com/media/files/yolov3.weights\")\n","  print(\"Converting yolov3.weights to yolo.h5...\")\n","  os.system(\"python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5\")\n","else:\n","  print(\"Model exist\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model exist\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-oIcqXmHAtwF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596506246955,"user_tz":-480,"elapsed":437,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}},"outputId":"302e3bc6-f197-40b7-ab4d-5de94eb2cda3"},"source":["# 直接下載 VOC2007 的資料集作為範例\n","if not os.path.exists(\"VOCdevkit\"):\n","  os.system(\"wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\") # 下載 VOC 資料集\n","  os.system(\"tar xvf VOCtrainval_06-Nov-2007.tar\") # 解壓縮資料集，會花幾分鐘\n","else:\n","  print(\"data exists\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["data exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UngKkY00Axh4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596506248260,"user_tz":-480,"elapsed":626,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}}},"source":["if not os.path.exists(\"2007_train.txt\"): # 範例中訓練模型時所使用的，已經做好轉換的 annotation 檔名，增加這個檢查避免每次重新跑這段轉換的程式碼\n","  import xml.etree.ElementTree as ET # 載入能夠 Parser xml 文件的 library\n","  from os import getcwd\n","\n","  sets=[('2007', 'train'), ('2007', 'val')]\n","\n","  # Pascal VOC 的資料類別\n","  classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n","\n","  # 把 annotation 轉換訓練時需要的資料形態\n","  def convert_annotation(year, image_id, list_file):\n","      in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id))\n","      tree=ET.parse(in_file)\n","      root = tree.getroot()\n","\n","      for obj in root.iter('object'):\n","          difficult = obj.find('difficult').text\n","          cls = obj.find('name').text\n","          if cls not in classes or int(difficult)==1:\n","              continue\n","          cls_id = classes.index(cls)\n","          xmlbox = obj.find('bndbox')\n","          b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n","          list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))\n","\n","  wd = \".\"\n","\n","  for year, image_set in sets:\n","      image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()\n","      annotation_path = '%s_%s.txt'%(year, image_set)\n","      list_file = open(annotation_path, 'w')\n","      print(\"save annotation at %s\" % annotation_path)\n","      for image_id in image_ids[:100]: # 只處理 100 張圖片來做範例\n","          list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg'%(wd, year, image_id))\n","          convert_annotation(year, image_id, list_file)\n","          list_file.write('\\n')\n","      list_file.close()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"X27JJXeDFYly","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596506252698,"user_tz":-480,"elapsed":641,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}}},"source":["# Q1 訓練模型時，資料集的格式是什麼: \n","# The format of 2007_train.txt and 2007_val.txt are\n","# image_id_path xmin ymin xmax ymax"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnOzzku5MIS_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596506291242,"user_tz":-480,"elapsed":629,"user":{"displayName":"李彥頲","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYLuy6N0x9fcWjTEPlNKqL6M1uCxO5ZzPgnC9q=s64","userId":"18319664738923439438"}},"outputId":"da6a2e19-2f8b-40a9-de6c-7c8c03647c75"},"source":["import numpy as np\n","import keras.backend as K\n","from keras.layers import Input, Lambda\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n","\n","from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n","from yolo3.utils import get_random_data\n","from train import get_classes, get_anchors, create_model, create_tiny_model, data_generator, data_generator_wrapper"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"g2ghFPJLInHN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8ad1ac13-5dfc-4ad9-a6c8-dc27d9775372"},"source":["annotation_path = '2007_train.txt' # 轉換好格式的標註檔案\n","log_dir = 'logs/000/' # 訓練好的模型儲存的路徑\n","classes_path = 'model_data/voc_classes.txt'\n","anchors_path = 'model_data/yolo_anchors.txt'\n","class_names = get_classes(classes_path)\n","num_classes = len(class_names)\n","anchors = get_anchors(anchors_path)\n","\n","input_shape = (416,416) # multiple of 32, hw\n","\n","is_tiny_version = len(anchors)==6 # default setting\n","if is_tiny_version:\n","    model = create_tiny_model(input_shape, anchors, num_classes,\n","        freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n","else:\n","    model = create_model(input_shape, anchors, num_classes,\n","        freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze\n","\n","logging = TensorBoard(log_dir=log_dir)\n","checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n","    monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n","\n","# 分為 training 以及 validation\n","val_split = 0.1\n","with open(annotation_path) as f:\n","    lines = f.readlines()\n","np.random.seed(10101)\n","np.random.shuffle(lines)\n","np.random.seed(None)\n","num_val = int(len(lines)*val_split)\n","num_train = len(lines) - num_val\n","\n","# Train with frozen layers first, to get a stable loss.\n","# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n","# 一開始先 freeze YOLO 除了 output layer 以外的 darknet53 backbone 來 train\n","if True:\n","    model.compile(optimizer=Adam(lr=1e-3), loss={\n","        # use custom yolo_loss Lambda layer.\n","        'yolo_loss': lambda y_true, y_pred: y_pred})\n","\n","    batch_size = 16\n","    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n","    # 模型利用 generator 產生的資料做訓練，強烈建議大家去閱讀及理解 data_generator_wrapper 在 train.py 中的實現\n","    \n","    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n","            steps_per_epoch=max(1, num_train//batch_size),\n","            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n","            validation_steps=max(1, num_val//batch_size),\n","            epochs=50,\n","            initial_epoch=0,\n","            callbacks=[logging, checkpoint])\n","    model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n","\n","# Unfreeze and continue training, to fine-tune.\n","# Train longer if the result is not good.\n","if True:\n","    # 把所有 layer 都改為 trainable\n","    for i in range(len(model.layers)):\n","        model.layers[i].trainable = True\n","    model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n","    print('Unfreeze all of the layers.')\n","\n","    batch_size = 16 # note that more GPU memory is required after unfreezing the body\n","    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n","    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n","        steps_per_epoch=max(1, num_train//batch_size),\n","        validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n","        validation_steps=max(1, num_val//batch_size),\n","        epochs=100,\n","        initial_epoch=50,\n","        callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n","    model.save_weights(log_dir + 'trained_weights_final.h5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","Create YOLOv3 model with 9 anchors and 20 classes.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 75) vs (255, 1024, 1, 1)).\n","  weight_values[i].shape))\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((75,) vs (255,)).\n","  weight_values[i].shape))\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 75) vs (255, 512, 1, 1)).\n","  weight_values[i].shape))\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((75,) vs (255,)).\n","  weight_values[i].shape))\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 75) vs (255, 256, 1, 1)).\n","  weight_values[i].shape))\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((75,) vs (255,)).\n","  weight_values[i].shape))\n"],"name":"stderr"},{"output_type":"stream","text":["Load weights model_data/yolo_weights.h5.\n","Freeze the first 249 layers of total 252 layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3080: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","Train on 90 samples, val on 10 samples, with batch size 16.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/50\n","5/5 [==============================] - 18s 4s/step - loss: 8466.0211 - val_loss: 5866.9277\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/50\n","5/5 [==============================] - 5s 901ms/step - loss: 4806.0962 - val_loss: 3335.4136\n","Epoch 3/50\n","5/5 [==============================] - 7s 1s/step - loss: 2736.2625 - val_loss: 1841.9579\n","Epoch 4/50\n","5/5 [==============================] - 2s 459ms/step - loss: 1619.6607 - val_loss: 1154.7094\n","Epoch 5/50\n","5/5 [==============================] - 2s 484ms/step - loss: 1039.3410 - val_loss: 774.4825\n","Epoch 6/50\n","5/5 [==============================] - 3s 602ms/step - loss: 713.7837 - val_loss: 562.7139\n","Epoch 7/50\n","5/5 [==============================] - 6s 1s/step - loss: 536.7273 - val_loss: 426.3079\n","Epoch 8/50\n","5/5 [==============================] - 7s 1s/step - loss: 407.8702 - val_loss: 372.2023\n","Epoch 9/50\n","5/5 [==============================] - 6s 1s/step - loss: 343.6635 - val_loss: 353.2449\n","Epoch 10/50\n","5/5 [==============================] - 6s 1s/step - loss: 299.2227 - val_loss: 263.4959\n","Epoch 11/50\n","5/5 [==============================] - 7s 1s/step - loss: 262.7155 - val_loss: 230.6769\n","Epoch 12/50\n","5/5 [==============================] - 6s 1s/step - loss: 243.9892 - val_loss: 233.1222\n","Epoch 13/50\n","5/5 [==============================] - 6s 1s/step - loss: 216.6080 - val_loss: 214.1174\n","Epoch 14/50\n","5/5 [==============================] - 7s 1s/step - loss: 199.5460 - val_loss: 187.4561\n","Epoch 15/50\n","5/5 [==============================] - 6s 1s/step - loss: 195.4031 - val_loss: 186.0423\n","Epoch 16/50\n","5/5 [==============================] - 6s 1s/step - loss: 179.3400 - val_loss: 192.2738\n","Epoch 17/50\n","5/5 [==============================] - 7s 1s/step - loss: 169.1999 - val_loss: 158.4084\n","Epoch 18/50\n","5/5 [==============================] - 7s 1s/step - loss: 161.4598 - val_loss: 156.3326\n","Epoch 19/50\n","5/5 [==============================] - 6s 1s/step - loss: 154.0918 - val_loss: 168.7073\n","Epoch 20/50\n","5/5 [==============================] - 7s 1s/step - loss: 145.0429 - val_loss: 144.7078\n","Epoch 21/50\n","5/5 [==============================] - 6s 1s/step - loss: 143.2179 - val_loss: 145.4642\n","Epoch 22/50\n","5/5 [==============================] - 6s 1s/step - loss: 133.6355 - val_loss: 137.7856\n","Epoch 23/50\n","5/5 [==============================] - 7s 1s/step - loss: 130.3520 - val_loss: 127.7483\n","Epoch 24/50\n","5/5 [==============================] - 6s 1s/step - loss: 124.4412 - val_loss: 116.7541\n","Epoch 25/50\n","5/5 [==============================] - 6s 1s/step - loss: 120.1471 - val_loss: 125.2920\n","Epoch 26/50\n","5/5 [==============================] - 7s 1s/step - loss: 109.6725 - val_loss: 110.1100\n","Epoch 27/50\n","5/5 [==============================] - 6s 1s/step - loss: 108.2588 - val_loss: 111.2000\n","Epoch 28/50\n","5/5 [==============================] - 6s 1s/step - loss: 108.9276 - val_loss: 113.0874\n","Epoch 29/50\n","5/5 [==============================] - 7s 1s/step - loss: 107.0794 - val_loss: 109.5112\n","Epoch 30/50\n","5/5 [==============================] - 6s 1s/step - loss: 100.5987 - val_loss: 96.6589\n","Epoch 31/50\n","5/5 [==============================] - 6s 1s/step - loss: 94.9594 - val_loss: 84.8910\n","Epoch 32/50\n","5/5 [==============================] - 7s 1s/step - loss: 96.4068 - val_loss: 98.5728\n","Epoch 33/50\n","5/5 [==============================] - 6s 1s/step - loss: 92.3229 - val_loss: 102.8566\n","Epoch 34/50\n","5/5 [==============================] - 6s 1s/step - loss: 87.7912 - val_loss: 94.8650\n","Epoch 35/50\n","5/5 [==============================] - 6s 1s/step - loss: 84.6702 - val_loss: 98.8720\n","Epoch 36/50\n","5/5 [==============================] - 6s 1s/step - loss: 85.2303 - val_loss: 87.2718\n","Epoch 37/50\n","5/5 [==============================] - 6s 1s/step - loss: 86.3831 - val_loss: 91.2660\n","Epoch 38/50\n","5/5 [==============================] - 7s 1s/step - loss: 80.9552 - val_loss: 92.0079\n","Epoch 39/50\n","5/5 [==============================] - 6s 1s/step - loss: 79.3538 - val_loss: 82.4210\n","Epoch 40/50\n","5/5 [==============================] - 6s 1s/step - loss: 79.4140 - val_loss: 84.9726\n","Epoch 41/50\n","5/5 [==============================] - 7s 1s/step - loss: 72.9172 - val_loss: 78.6586\n","Epoch 42/50\n","5/5 [==============================] - 6s 1s/step - loss: 75.6047 - val_loss: 84.7340\n","Epoch 43/50\n","5/5 [==============================] - 6s 1s/step - loss: 72.6946 - val_loss: 88.9536\n","Epoch 44/50\n","5/5 [==============================] - 6s 1s/step - loss: 69.8798 - val_loss: 70.3286\n","Epoch 45/50\n","5/5 [==============================] - 6s 1s/step - loss: 71.3994 - val_loss: 80.4835\n","Epoch 46/50\n","5/5 [==============================] - 6s 1s/step - loss: 68.7442 - val_loss: 68.6877\n","Epoch 47/50\n","5/5 [==============================] - 7s 1s/step - loss: 68.0961 - val_loss: 72.3768\n","Epoch 48/50\n","5/5 [==============================] - 7s 1s/step - loss: 66.4708 - val_loss: 79.2159\n","Epoch 49/50\n","5/5 [==============================] - 6s 1s/step - loss: 63.4983 - val_loss: 60.1620\n","Epoch 50/50\n","5/5 [==============================] - 7s 1s/step - loss: 65.6928 - val_loss: 76.5856\n","Unfreeze all of the layers.\n","Train on 90 samples, val on 10 samples, with batch size 16.\n","Epoch 51/100\n","5/5 [==============================] - 29s 6s/step - loss: 49.2050 - val_loss: 69.6508\n","Epoch 52/100\n","5/5 [==============================] - 6s 1s/step - loss: 35.6779 - val_loss: 56.1842\n","Epoch 53/100\n","5/5 [==============================] - 6s 1s/step - loss: 37.0376 - val_loss: 55.4495\n","Epoch 54/100\n","5/5 [==============================] - 6s 1s/step - loss: 33.6866 - val_loss: 37.3346\n","Epoch 55/100\n","5/5 [==============================] - 6s 1s/step - loss: 29.2687 - val_loss: 47.4790\n","Epoch 56/100\n","5/5 [==============================] - 8s 2s/step - loss: 31.7693 - val_loss: 43.1491\n","Epoch 57/100\n","1/5 [=====>........................] - ETA: 5s - loss: 30.3054"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kcqtm9sGInoc","colab_type":"code","colab":{}},"source":["# 如何使用已經訓練好的模型\n","# load the trained model weightings\n","\n","yolo_model = YOLO(model_path='logs/000/trained_weights_final.h5', classes_path=\"model_data/voc_classes.txt\")\n","r_image = yolo_model.detect_image(image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Q6RaKnwNskY","colab_type":"code","colab":{}},"source":["r_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JB9BaHJJNukE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}